{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARNqkpwDXzL8"
   },
   "source": [
    "# Dental Implant Classification - Systematic Testing\n",
    "\n",
    "This notebook provides a comprehensive environment for testing different combinations of:\n",
    "1. Data sources\n",
    "2. Image processing techniques\n",
    "3. Model architectures\n",
    "\n",
    "The goal is to identify the optimal approach for dental implant classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdwUuTNZX1w9",
    "outputId": "c332805e-e9be-4a4a-87b3-8f7e6ab8c48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.14.1)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-image tensorflow matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYhiSdKCXtwj"
   },
   "source": [
    "## 1. Mount Google Drive\n",
    "Mount Google Drive to access the dental implant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyQaCC7ee4BT",
    "outputId": "7af13f51-3bda-4cda-db0f-ae8a840a7d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Import the os module for file system operations\n",
    "import os\n",
    "\n",
    "# Check if the mount point directory already exists\n",
    "if os.path.exists('/content/drive'):\n",
    "  # If it exists, remove it and its contents\n",
    "  !rm -rf /content/drive\n",
    "\n",
    "# Now, mount your Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIs3oAgsX1I3"
   },
   "source": [
    "## 2. Forward Declarations\n",
    "\n",
    "Define forward declarations for all functions and widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "byDwAQBsLisR"
   },
   "outputs": [],
   "source": [
    "# Add this cell at the very beginning of your notebook\n",
    "# IMPORTANT: Define forward declarations for all functions and widgets\n",
    "\n",
    "# 1. Function forward declarations\n",
    "def display_hyperparameter_controls():\n",
    "    \"\"\"Forward declaration for hyperparameter controls\"\"\"\n",
    "    # This will be replaced when the actual function is defined\n",
    "    print(\"Loading hyperparameter controls...\")\n",
    "\n",
    "    # Create sliders\n",
    "    lr_slider = widgets.FloatLogSlider(\n",
    "        value=1e-3,\n",
    "        base=10,\n",
    "        min=-5,  # 1e-5\n",
    "        max=-1,  # 1e-1\n",
    "        step=0.2,\n",
    "        description='Learning Rate:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    # Display basic controls\n",
    "    display(lr_slider)\n",
    "    print(\"Please run the complete hyperparameter cell for all controls\")\n",
    "\n",
    "def on_source_change(change):\n",
    "    \"\"\"Forward declaration for source change handler\"\"\"\n",
    "    pass\n",
    "\n",
    "def on_processing_change(change):\n",
    "    \"\"\"Forward declaration for processing change handler\"\"\"\n",
    "    pass\n",
    "\n",
    "def scan_data_sources():\n",
    "    \"\"\"Forward declaration\"\"\"\n",
    "    return []\n",
    "\n",
    "def display_processing_comparison(source, method):\n",
    "    \"\"\"Forward declaration\"\"\"\n",
    "    print(f\"Showing comparison between original and {method} images...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk7R_R_WX3ZJ"
   },
   "source": [
    "## 3. Setup and Imports\n",
    "Let's import required libraries and set directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "iZunFKFXX5LJ",
    "outputId": "de7a2330-82ea-4747-f069-a89f05d1ff6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Widgets initialized.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB3, ResNet50, DenseNet121\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import cv2\n",
    "from skimage import io, color, exposure, filters, util\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Set base paths\n",
    "BASE_PATH = '/content/drive/MyDrive/dental_implant_project'\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data_collected')\n",
    "RESULTS_PATH = os.path.join(BASE_PATH, 'results')\n",
    "\n",
    "# Create results directory structure if it doesn't exist\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'logs'), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'metrics'), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'plots'), exist_ok=True)\n",
    "\n",
    "# Configuration settings dictionary\n",
    "config = {\n",
    "    'data_source': None,\n",
    "    'processing_method': 'original',\n",
    "    'model_type': 'efficientnetb3',\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 10,\n",
    "    'img_channels': 3,  # Will be set based on processing method\n",
    "    'input_shape': None,  # Will be determined from data\n",
    "    'num_classes': None,  # Will be determined from data\n",
    "    'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "}\n",
    "\n",
    "# 2. Create widgets (initialize them properly)\n",
    "# Initialize processing dropdown properly\n",
    "processing_dropdown = widgets.Dropdown(\n",
    "    options=['original', 'denoised', 'enhanced', 'sharpened'],\n",
    "    description='Processing:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Initialize model selection dropdown\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=['efficientnetb3', 'custom_cnn', 'resnet50', 'densenet121'],\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Initialize force selection button\n",
    "force_selection_button = widgets.Button(\n",
    "    description='Select Source',\n",
    "    button_style='primary',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "print(\"Setup complete! Widgets initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcVAXtoVX7H_"
   },
   "source": [
    "## 4. Data Source Selection\n",
    "\n",
    "Let's scan available data sources in the Google Drive and select which one to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "96d4c0bc4de14d0bb5de873e82fc71e2",
      "575d210aab7242c78025d6d3facd2f19",
      "a7a8ab20b9db4170bf5b6df48a7df7ae",
      "cf27a56e397f4a8e99443242f3dc95e1",
      "17727b275068471db85804b7100c022c",
      "46df622d695c46c2b0d2d1d50f8bf960",
      "11946e19fd8c4612a67f1185dcb1210a",
      "9a4f3c8e22824b36b4b5ae995ac1d9d5",
      "aa227c2480c144d8b4a450eb13b0803e",
      "7cf69310f4bb46399e7f9ee06220096a",
      "0605ed1c631d46a2846a59fe990fece2",
      "7d760f0ce6504190a8f2d1d41bb773db",
      "be9bdccd653d40e18a91ae7c1eb4fa8f",
      "9e7ab42970e64b0bbba98f5b73ef559d",
      "8c819e3ded474b2097641ffed2034177",
      "9d906ccb068d420c87fb30e776bd3cbc",
      "9753985ae17e4de1a86238b565cff1aa",
      "fff2bc3bf80e4303b24324477233ccf1"
     ]
    },
    "id": "pvKcdgCSX8k0",
    "outputId": "ab270b5e-c5a3-448e-baa6-25b5d11ade48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected data source: IMPLANT SYSTEM DETECTION.v7i.yolov5pytorch\n",
      "Selected processing method: original\n",
      "Selected model: custom_cnn\n",
      "Loading hyperparameter controls...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d906ccb068d420c87fb30e776bd3cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatLogSlider(value=0.001, continuous_update=False, description='Learning Rate:', max=-1.0, min=-5.0, step=0.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please run the complete hyperparameter cell for all controls\n"
     ]
    }
   ],
   "source": [
    "def scan_data_sources():\n",
    "    \"\"\"Scan for available data sources in the data directory\"\"\"\n",
    "    sources = []\n",
    "\n",
    "    # Look for directories in the data_collected folder\n",
    "    for item in os.listdir(DATA_PATH):\n",
    "        source_path = os.path.join(DATA_PATH, item)\n",
    "        if os.path.isdir(source_path):\n",
    "            # Check if it contains train/val/test subdirectories\n",
    "            if all(os.path.isdir(os.path.join(source_path, split)) for split in ['train', 'val', 'test']):\n",
    "                sources.append(item)\n",
    "\n",
    "    return sources\n",
    "\n",
    "def get_class_distribution(data_dir):\n",
    "    \"\"\"Get number of images per class\"\"\"\n",
    "    class_counts = {}\n",
    "\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "            class_counts[class_name] = count\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "def display_source_info(source):\n",
    "    \"\"\"Display information about the selected data source\"\"\"\n",
    "    source_path = os.path.join(DATA_PATH, source)\n",
    "\n",
    "    # Get class distribution for each split\n",
    "    splits = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(source_path, split)\n",
    "        class_counts = get_class_distribution(split_path)\n",
    "        splits[split] = class_counts\n",
    "\n",
    "    # Calculate total images\n",
    "    total_images = sum(sum(counts.values()) for counts in splits.values())\n",
    "\n",
    "    # Display summary\n",
    "    print(f\"=== Data Source: {source} ===\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "\n",
    "    # Display class distribution\n",
    "    for split, counts in splits.items():\n",
    "        print(f\"\\n{split.capitalize()} set:\")\n",
    "        for class_name, count in counts.items():\n",
    "            print(f\"  {class_name}: {count} images\")\n",
    "\n",
    "    # Get image sizes\n",
    "    train_path = os.path.join(source_path, 'train')\n",
    "    class_dirs = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
    "\n",
    "    if class_dirs:\n",
    "        first_class = class_dirs[0]\n",
    "        class_path = os.path.join(train_path, first_class)\n",
    "        image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        if image_files:\n",
    "            sample_img_path = os.path.join(class_path, image_files[0])\n",
    "            img = plt.imread(sample_img_path)\n",
    "            print(f\"\\nImage dimensions: {img.shape}\")\n",
    "\n",
    "            # Set the input shape based on the first image\n",
    "            if len(img.shape) == 2:  # Grayscale\n",
    "                config['input_shape'] = (img.shape[0], img.shape[1], 1)\n",
    "                config['img_channels'] = 1\n",
    "            else:  # RGB\n",
    "                config['input_shape'] = img.shape\n",
    "                config['img_channels'] = img.shape[2]\n",
    "\n",
    "    # Display sample images\n",
    "    display_sample_images(source)\n",
    "\n",
    "    # Set number of classes\n",
    "    config['num_classes'] = len(splits['train'])\n",
    "\n",
    "    return splits\n",
    "\n",
    "def display_sample_images(source, num_samples=3):\n",
    "    \"\"\"Display sample images from each class in the selected source\"\"\"\n",
    "    train_path = os.path.join(DATA_PATH, source, 'train')\n",
    "    classes = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
    "\n",
    "    num_classes = len(classes)\n",
    "    fig, axes = plt.subplots(num_classes, num_samples, figsize=(15, 3*num_classes))\n",
    "\n",
    "    # Handle the case where there's only one class\n",
    "    if num_classes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(train_path, class_name)\n",
    "        image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        # Select random samples\n",
    "        import random\n",
    "        samples = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "\n",
    "        for j, file in enumerate(samples):\n",
    "            img_path = os.path.join(class_path, file)\n",
    "            img = plt.imread(img_path)\n",
    "\n",
    "            # Handle grayscale vs RGB display\n",
    "            if len(img.shape) == 2:\n",
    "                axes[i][j].imshow(img, cmap='gray')\n",
    "            else:\n",
    "                axes[i][j].imshow(img)\n",
    "\n",
    "            axes[i][j].set_title(f\"{class_name}\\n{file}\")\n",
    "            axes[i][j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize data sources dropdown properly\n",
    "sources = scan_data_sources()\n",
    "source_dropdown = widgets.Dropdown(\n",
    "    options=sources,\n",
    "    description='Data Source:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "def on_source_change(change):\n",
    "    \"\"\"Handle data source selection change\"\"\"\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        selected_source = change['new']\n",
    "        config['data_source'] = selected_source\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Selected data source: {selected_source}\")\n",
    "\n",
    "        # Display source information\n",
    "        display_source_info(selected_source)\n",
    "\n",
    "        # Show processing dropdown\n",
    "        display(processing_dropdown)\n",
    "\n",
    "# Attach observer to source dropdown\n",
    "source_dropdown.observe(on_source_change, names='value')\n",
    "\n",
    "def force_selection(b=None):\n",
    "    \"\"\"Force the selection of the current dropdown value to trigger the change event\"\"\"\n",
    "    selected_source = source_dropdown.value\n",
    "    if selected_source:\n",
    "        config['data_source'] = selected_source\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Selected data source: {selected_source}\")\n",
    "\n",
    "        # Display source information\n",
    "        display_source_info(selected_source)\n",
    "\n",
    "        # Show processing dropdown\n",
    "        display(processing_dropdown)\n",
    "\n",
    "# Attach the click handler to the force selection button\n",
    "force_selection_button.on_click(force_selection)\n",
    "\n",
    "# Display source selection controls\n",
    "display(source_dropdown)\n",
    "display(force_selection_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFmXKKIyX_UJ"
   },
   "source": [
    "## 5. Image Processing Options\n",
    "\n",
    "Define different processing techniques to apply to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T6PzG8q7YAVL"
   },
   "outputs": [],
   "source": [
    "# Image processing functions\n",
    "def apply_processing(img, method='original'):\n",
    "    \"\"\"Apply selected processing method to an image\"\"\"\n",
    "    # Convert to float for processing if not already\n",
    "    if img.dtype != np.float32 and img.dtype != np.float64:\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    if method == 'original':\n",
    "        # No processing\n",
    "        processed = img\n",
    "\n",
    "    elif method == 'denoised':\n",
    "        # Apply denoising\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:  # RGB\n",
    "            processed = cv2.fastNlMeansDenoisingColored(\n",
    "                (img * 255).astype(np.uint8), None, 10, 10, 7, 21)\n",
    "            processed = processed.astype(np.float32) / 255.0\n",
    "        else:  # Grayscale\n",
    "            if len(img.shape) == 3:  # Extra dimension\n",
    "                img_gray = img[:,:,0]\n",
    "            else:\n",
    "                img_gray = img\n",
    "            processed = cv2.fastNlMeansDenoising(\n",
    "                (img_gray * 255).astype(np.uint8), None, 10, 7, 21)\n",
    "            processed = processed.astype(np.float32) / 255.0\n",
    "\n",
    "            # Restore shape if needed\n",
    "            if len(img.shape) == 3 and processed.ndim == 2:\n",
    "                processed = np.expand_dims(processed, axis=2)\n",
    "\n",
    "    elif method == 'enhanced':\n",
    "        # Apply contrast enhancement\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:  # RGB\n",
    "            img_lab = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2LAB)\n",
    "            l, a, b = cv2.split(img_lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "            cl = clahe.apply(l)\n",
    "            processed_lab = cv2.merge((cl, a, b))\n",
    "            processed = cv2.cvtColor(processed_lab, cv2.COLOR_LAB2RGB)\n",
    "            processed = processed.astype(np.float32) / 255.0\n",
    "        else:  # Grayscale\n",
    "            if len(img.shape) == 3:  # Extra dimension\n",
    "                img_gray = img[:,:,0]\n",
    "            else:\n",
    "                img_gray = img\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "            processed = clahe.apply((img_gray * 255).astype(np.uint8))\n",
    "            processed = processed.astype(np.float32) / 255.0\n",
    "\n",
    "            # Restore shape if needed\n",
    "            if len(img.shape) == 3 and processed.ndim == 2:\n",
    "                processed = np.expand_dims(processed, axis=2)\n",
    "\n",
    "    elif method == 'sharpened':\n",
    "        # Apply sharpening\n",
    "        kernel = np.array([[-1, -1, -1],\n",
    "                          [-1,  9, -1],\n",
    "                          [-1, -1, -1]])\n",
    "\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:  # RGB\n",
    "            processed = cv2.filter2D((img * 255).astype(np.uint8), -1, kernel)\n",
    "            processed = processed.astype(np.float32) / 255.0\n",
    "        else:  # Grayscale\n",
    "            if len(img.shape) == 3:  # Extra dimension\n",
    "                img_gray = img[:,:,0]\n",
    "            else:\n",
    "                img_gray = img\n",
    "            processed = cv2.filter2D((img_gray * 255).astype(np.uint8), -1, kernel)\n",
    "            processed = processed.astype(np.float32) / 255.0\n",
    "\n",
    "            # Restore shape if needed\n",
    "            if len(img.shape) == 3 and processed.ndim == 2:\n",
    "                processed = np.expand_dims(processed, axis=2)\n",
    "\n",
    "    # Ensure values are in [0, 1] range\n",
    "    processed = np.clip(processed, 0, 1)\n",
    "\n",
    "    return processed\n",
    "\n",
    "def display_processing_comparison(source, method):\n",
    "    \"\"\"Display comparison of original and processed images\"\"\"\n",
    "    if not source:\n",
    "        print(\"Please select a data source first.\")\n",
    "        return\n",
    "\n",
    "    train_path = os.path.join(DATA_PATH, source, 'train')\n",
    "    classes = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
    "\n",
    "    # Select one image from each class\n",
    "    fig, axes = plt.subplots(len(classes), 2, figsize=(10, 4*len(classes)))\n",
    "\n",
    "    # Handle case with only one class\n",
    "    if len(classes) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(train_path, class_name)\n",
    "        image_files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        if image_files:\n",
    "            # Select first image\n",
    "            img_path = os.path.join(class_path, image_files[0])\n",
    "            img = plt.imread(img_path)\n",
    "\n",
    "            # Convert to float for processing if not already\n",
    "            if img.dtype != np.float32 and img.dtype != np.float64:\n",
    "                img = img.astype(np.float32)\n",
    "                if img.max() > 1.0:\n",
    "                    img = img / 255.0\n",
    "\n",
    "            # Apply processing\n",
    "            processed_img = apply_processing(img, method)\n",
    "\n",
    "            # Display original\n",
    "            if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):\n",
    "                axes[i][0].imshow(img, cmap='gray')\n",
    "            else:\n",
    "                axes[i][0].imshow(img)\n",
    "            axes[i][0].set_title(f\"{class_name} - Original\")\n",
    "            axes[i][0].axis('off')\n",
    "\n",
    "            # Display processed\n",
    "            if len(processed_img.shape) == 2 or (len(processed_img.shape) == 3 and processed_img.shape[2] == 1):\n",
    "                axes[i][1].imshow(processed_img, cmap='gray')\n",
    "            else:\n",
    "                axes[i][1].imshow(processed_img)\n",
    "            axes[i][1].set_title(f\"{class_name} - {method.capitalize()}\")\n",
    "            axes[i][1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Add a continue button for proper workflow\n",
    "    continue_button = widgets.Button(\n",
    "        description='Continue to Model Selection',\n",
    "        button_style='success',\n",
    "        icon='arrow-right'\n",
    "    )\n",
    "\n",
    "    def on_continue_click(b):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Selected data source: {config['data_source']}\")\n",
    "        print(f\"Selected processing method: {config['processing_method']}\")\n",
    "\n",
    "        # Define model dropdown handler\n",
    "        def on_model_change(change):\n",
    "            if change['type'] == 'change' and change['name'] == 'value':\n",
    "                selected_model = change['new']\n",
    "                config['model_type'] = selected_model\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Selected data source: {config['data_source']}\")\n",
    "                print(f\"Selected processing method: {config['processing_method']}\")\n",
    "                print(f\"Selected model: {selected_model}\")\n",
    "\n",
    "                # Show hyperparameter controls after model is selected\n",
    "                display_hyperparameter_controls()\n",
    "\n",
    "        model_dropdown.observe(on_model_change, names='value')\n",
    "        display(model_dropdown)\n",
    "\n",
    "    continue_button.on_click(on_continue_click)\n",
    "    display(continue_button)\n",
    "\n",
    "def on_processing_change(change):\n",
    "    \"\"\"Handle processing method selection change\"\"\"\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        selected_method = change['new']\n",
    "        config['processing_method'] = selected_method\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Selected data source: {config['data_source']}\")\n",
    "        print(f\"Selected processing method: {selected_method}\")\n",
    "\n",
    "        # Display comparison of original vs processed\n",
    "        display_processing_comparison(config['data_source'], selected_method)\n",
    "\n",
    "# Attach observer to processing dropdown\n",
    "processing_dropdown.observe(on_processing_change, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0d1e42RYDcb"
   },
   "source": [
    "## 6. Model Selection and Building\n",
    "\n",
    "Define model architectures and create a selection dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yvDgWLlzYEh6"
   },
   "outputs": [],
   "source": [
    "def build_efficientnetb3_model(input_shape, num_classes):\n",
    "    \"\"\"Build EfficientNetB3 model\"\"\"\n",
    "    # Use transfer learning for efficient training\n",
    "    base_model = EfficientNetB3(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Freeze base model for initial training\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create classification head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_custom_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"Build custom CNN model\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_resnet50_model(input_shape, num_classes):\n",
    "    \"\"\"Build ResNet50 model\"\"\"\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Freeze base model for initial training\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create classification head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_densenet121_model(input_shape, num_classes):\n",
    "    \"\"\"Build DenseNet121 model\"\"\"\n",
    "    base_model = DenseNet121(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Freeze base model for initial training\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create classification head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model(model_type, input_shape, num_classes):\n",
    "    \"\"\"Get the specified model architecture\"\"\"\n",
    "    if model_type == 'efficientnetb3':\n",
    "        return build_efficientnetb3_model(input_shape, num_classes)\n",
    "    elif model_type == 'custom_cnn':\n",
    "        return build_custom_cnn_model(input_shape, num_classes)\n",
    "    elif model_type == 'resnet50':\n",
    "        return build_resnet50_model(input_shape, num_classes)\n",
    "    elif model_type == 'densenet121':\n",
    "        return build_densenet121_model(input_shape, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Create dropdown for model selection\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=['efficientnetb3', 'custom_cnn', 'resnet50', 'densenet121'],\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWMjESU_YHXJ"
   },
   "source": [
    "## 7. Training Configuration and Hyperparameters\n",
    "\n",
    "Allow the user to configure training hyperparameters and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-jsWniqiYIY0"
   },
   "outputs": [],
   "source": [
    "def display_hyperparameter_controls():\n",
    "    \"\"\"Display sliders for hyperparameter configuration\"\"\"\n",
    "    # Learning rate slider\n",
    "    lr_slider = widgets.FloatLogSlider(\n",
    "        value=1e-3,\n",
    "        base=10,\n",
    "        min=-5,  # 1e-5\n",
    "        max=-1,  # 1e-1\n",
    "        step=0.2,\n",
    "        description='Learning Rate:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    # Batch size slider\n",
    "    batch_slider = widgets.IntSlider(\n",
    "        value=16,\n",
    "        min=4,\n",
    "        max=64,\n",
    "        step=4,\n",
    "        description='Batch Size:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    # Epochs slider\n",
    "    epochs_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=50,\n",
    "        step=1,\n",
    "        description='Epochs:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    # Training button\n",
    "    train_button = widgets.Button(\n",
    "        description='Start Training',\n",
    "        button_style='success',\n",
    "        icon='play'\n",
    "    )\n",
    "\n",
    "    # Update config when sliders change\n",
    "    def on_lr_change(change):\n",
    "        config['learning_rate'] = change['new']\n",
    "        print(f\"Learning rate set to: {change['new']}\")\n",
    "\n",
    "    def on_batch_change(change):\n",
    "        config['batch_size'] = change['new']\n",
    "        print(f\"Batch size set to: {change['new']}\")\n",
    "\n",
    "    def on_epochs_change(change):\n",
    "        config['epochs'] = change['new']\n",
    "        print(f\"Epochs set to: {change['new']}\")\n",
    "\n",
    "    def on_train_click(b):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Starting training with the following configuration:\")\n",
    "        print(f\"- Data source: {config['data_source']}\")\n",
    "        print(f\"- Processing method: {config['processing_method']}\")\n",
    "        print(f\"- Model: {config['model_type']}\")\n",
    "        print(f\"- Learning rate: {config['learning_rate']}\")\n",
    "        print(f\"- Batch size: {config['batch_size']}\")\n",
    "        print(f\"- Epochs: {config['epochs']}\")\n",
    "        print(f\"- Input shape: {config['input_shape']}\")\n",
    "        print(f\"- Number of classes: {config['num_classes']}\")\n",
    "\n",
    "        # Start training\n",
    "        train_model()\n",
    "\n",
    "    lr_slider.observe(on_lr_change, names='value')\n",
    "    batch_slider.observe(on_batch_change, names='value')\n",
    "    epochs_slider.observe(on_epochs_change, names='value')\n",
    "    train_button.on_click(on_train_click)\n",
    "\n",
    "    # Display controls\n",
    "    display(lr_slider, batch_slider, epochs_slider, train_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gq9XbxweYKvZ"
   },
   "source": [
    "## 8. Data Generators and Training Pipeline\n",
    "\n",
    "Create data generators with the specified processing and train the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeMWYkWlYLtq"
   },
   "outputs": [],
   "source": [
    "def create_data_generators():\n",
    "    \"\"\"Create data generators with processing\"\"\"\n",
    "    if not config['data_source']:\n",
    "        print(\"Please select a data source.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Paths to data splits\n",
    "    train_dir = os.path.join(DATA_PATH, config['data_source'], 'train')\n",
    "    val_dir = os.path.join(DATA_PATH, config['data_source'], 'val')\n",
    "    test_dir = os.path.join(DATA_PATH, config['data_source'], 'test')\n",
    "\n",
    "    # Define preprocessing function based on selected method\n",
    "    def preprocess_fn(img):\n",
    "        return apply_processing(img, config['processing_method'])\n",
    "\n",
    "    # Create data generators\n",
    "    if config['processing_method'] == 'original':\n",
    "        # For original images, use standard rescaling\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            validation_split=0.0  # We have a separate validation set\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    else:\n",
    "        # For other processing methods, use preprocessing function\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_fn,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            validation_split=0.0\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(preprocessing_function=preprocess_fn)\n",
    "        test_datagen = ImageDataGenerator(preprocessing_function=preprocess_fn)\n",
    "\n",
    "    # Create generators\n",
    "    color_mode = 'grayscale' if config['img_channels'] == 1 else 'rgb'\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(config['input_shape'][0], config['input_shape'][1]),\n",
    "        batch_size=config['batch_size'],\n",
    "        class_mode='categorical',\n",
    "        color_mode=color_mode,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(config['input_shape'][0], config['input_shape'][1]),\n",
    "        batch_size=config['batch_size'],\n",
    "        class_mode='categorical',\n",
    "        color_mode=color_mode,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(config['input_shape'][0], config['input_shape'][1]),\n",
    "        batch_size=config['batch_size'],\n",
    "        class_mode='categorical',\n",
    "        color_mode=color_mode,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Update class indices in config\n",
    "    config['class_indices'] = train_generator.class_indices\n",
    "\n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Train the model with selected configuration\"\"\"\n",
    "    # Create data generators\n",
    "    train_generator, val_generator, test_generator = create_data_generators()\n",
    "\n",
    "    if not train_generator:\n",
    "        return\n",
    "\n",
    "    # Build model\n",
    "    model = get_model(\n",
    "        model_type=config['model_type'],\n",
    "        input_shape=config['input_shape'],\n",
    "        num_classes=config['num_classes']\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=config['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Create results subdirectory\n",
    "    timestamp = config['timestamp']\n",
    "    experiment_name = f\"{config['data_source']}_{config['processing_method']}_{config['model_type']}_{timestamp}\"\n",
    "    experiment_dir = os.path.join(RESULTS_PATH, experiment_name)\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    # Create callbacks\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(RESULTS_PATH, 'models', f\"{experiment_name}_best.keras\"),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=config['epochs'],\n",
    "        validation_data=val_generator,\n",
    "        callbacks=[model_checkpoint, early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(RESULTS_PATH, 'models', f\"{experiment_name}_final.keras\")\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\nFinal model saved to: {final_model_path}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history, experiment_name)\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_model(model, test_generator, experiment_name)\n",
    "\n",
    "    # Save configuration\n",
    "    save_config(experiment_name)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnB01CdpYOnp"
   },
   "source": [
    "## 9. Visualization and Evaluation\n",
    "\n",
    "Functions for plotting training history, evaluating model performance, and saving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OosVTSYiYPpl"
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, experiment_name):\n",
    "    \"\"\"Plot and save training history\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plt_path = os.path.join(RESULTS_PATH, 'plots', f\"{experiment_name}_history.png\")\n",
    "    plt.savefig(plt_path)\n",
    "    plt.show()\n",
    "\n",
    "    # Save history data\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(RESULTS_PATH, 'logs', f\"{experiment_name}_history.csv\"))\n",
    "\n",
    "def evaluate_model(model, test_generator, experiment_name):\n",
    "    \"\"\"Evaluate model on test set and save metrics\"\"\"\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = model.predict(test_generator)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # True labels\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    # Get class names\n",
    "    class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save confusion matrix\n",
    "    cm_path = os.path.join(RESULTS_PATH, 'plots', f\"{experiment_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path)\n",
    "    plt.show()\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    # Save report\n",
    "    report_path = os.path.join(RESULTS_PATH, 'metrics', f\"{experiment_name}_classification_report.csv\")\n",
    "    report_df.to_csv(report_path)\n",
    "\n",
    "    # Display report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(pd.DataFrame(report).transpose())\n",
    "\n",
    "    # Save test metrics\n",
    "    metrics = {\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(RESULTS_PATH, 'metrics', f\"{experiment_name}_metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    # Plot ROC curves for multi-class\n",
    "    plot_roc_curves(y_true, predictions, class_names, experiment_name)\n",
    "\n",
    "def plot_roc_curves(y_true, y_pred_proba, class_names, experiment_name):\n",
    "    \"\"\"Plot ROC curves for each class\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # One-hot encode true labels\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(len(class_names)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    # Plot random guess line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Save ROC curves\n",
    "    roc_path = os.path.join(RESULTS_PATH, 'plots', f\"{experiment_name}_roc_curves.png\")\n",
    "    plt.savefig(roc_path)\n",
    "    plt.show()\n",
    "\n",
    "def save_config(experiment_name):\n",
    "    \"\"\"Save experiment configuration\"\"\"\n",
    "    config_copy = config.copy()\n",
    "\n",
    "    # Convert input_shape to list for JSON serialization\n",
    "    if config_copy['input_shape'] is not None:\n",
    "        config_copy['input_shape'] = list(config_copy['input_shape'])\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(RESULTS_PATH, 'logs', f\"{experiment_name}_config.json\"), 'w') as f:\n",
    "        json.dump(config_copy, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EXnVhnbYTfH"
   },
   "source": [
    "## 10. Feature Visualization and Model Interpretation\n",
    "\n",
    "Visualize what the model is focusing on in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqudrObxYUeB"
   },
   "outputs": [],
   "source": [
    "def visualize_model_attention(model, img_path, experiment_name):\n",
    "    \"\"\"Visualize what the model is focusing on using Grad-CAM\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize image to match input shape\n",
    "    img_resized = cv2.resize(img, (config['input_shape'][1], config['input_shape'][0]))\n",
    "\n",
    "    # Preprocess image\n",
    "    if config['img_channels'] == 1:\n",
    "        # Convert to grayscale if needed\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)\n",
    "        img_resized = np.expand_dims(img_resized, axis=-1)\n",
    "\n",
    "    # Apply selected processing\n",
    "    img_processed = apply_processing(img_resized / 255.0, config['processing_method'])\n",
    "\n",
    "    # Add batch dimension\n",
    "    img_batch = np.expand_dims(img_processed, axis=0)\n",
    "\n",
    "    # Get model prediction\n",
    "    predictions = model.predict(img_batch)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "    # Find the last convolutional layer\n",
    "    last_conv_layer = None\n",
    "    for layer in reversed(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            last_conv_layer = layer\n",
    "            break\n",
    "\n",
    "    if last_conv_layer is None:\n",
    "        print(\"Could not find convolutional layer for visualization.\")\n",
    "        return\n",
    "\n",
    "    # Create Grad-CAM\n",
    "    grad_model = Model(\n",
    "        inputs=[model.inputs],\n",
    "        outputs=[last_conv_layer.output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_batch)\n",
    "        class_output = predictions[:, predicted_class]\n",
    "\n",
    "    grads = tape.gradient(class_output, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "\n",
    "    for i in range(pooled_grads.shape[0]):\n",
    "        conv_outputs[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    heatmap = tf.reduce_mean(conv_outputs, axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)\n",
    "    heatmap = heatmap.numpy()\n",
    "\n",
    "    # Resize heatmap to original image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Apply colormap to heatmap\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Overlay heatmap on original image\n",
    "    alpha = 0.4\n",
    "    superimposed_img = cv2.addWeighted(\n",
    "        cv2.cvtColor(img, cv2.COLOR_RGB2BGR),\n",
    "        1 - alpha,\n",
    "        heatmap,\n",
    "        alpha,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Display original and heatmap overlay\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Grad-CAM Heatmap')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save visualization\n",
    "    viz_path = os.path.join(RESULTS_PATH, 'plots', f\"{experiment_name}_gradcam.png\")\n",
    "    plt.savefig(viz_path)\n",
    "    plt.show()\n",
    "\n",
    "    return heatmap, superimposed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unnt2lrvYWyG"
   },
   "source": [
    "## 11. Experiment Comparison\n",
    "\n",
    "Compare results across different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPiuBh54YXvz"
   },
   "outputs": [],
   "source": [
    "def list_experiments():\n",
    "    \"\"\"List all available experiments for comparison\"\"\"\n",
    "    metric_files = [f for f in os.listdir(os.path.join(RESULTS_PATH, 'metrics'))\n",
    "                   if f.endswith('_metrics.json')]\n",
    "\n",
    "    experiment_names = [f.replace('_metrics.json', '') for f in metric_files]\n",
    "    return sorted(experiment_names)\n",
    "\n",
    "def compare_experiments(experiment_names):\n",
    "    \"\"\"Compare results from multiple experiments\"\"\"\n",
    "    if not experiment_names or len(experiment_names) < 2:\n",
    "        print(\"Please provide at least two experiment names to compare.\")\n",
    "        return\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    for name in experiment_names:\n",
    "        metrics_path = os.path.join(RESULTS_PATH, 'metrics', f\"{name}_metrics.json\")\n",
    "        if not os.path.exists(metrics_path):\n",
    "            print(f\"Metrics file not found for experiment: {name}\")\n",
    "            continue\n",
    "\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "        # Load config\n",
    "        config_path = os.path.join(RESULTS_PATH, 'logs', f\"{name}_config.json\")\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                exp_config = json.load(f)\n",
    "        else:\n",
    "            exp_config = {}\n",
    "\n",
    "        # Extract key information\n",
    "        experiment_info = {\n",
    "            'name': name,\n",
    "            'source': exp_config.get('data_source', 'unknown'),\n",
    "            'processing': exp_config.get('processing_method', 'unknown'),\n",
    "            'model': exp_config.get('model_type', 'unknown'),\n",
    "            'accuracy': metrics.get('test_accuracy', 0),\n",
    "            'loss': metrics.get('test_loss', 0)\n",
    "        }\n",
    "\n",
    "        # Extract f1-scores from classification report\n",
    "        if 'classification_report' in metrics:\n",
    "            report = metrics['classification_report']\n",
    "            if 'weighted avg' in report:\n",
    "                experiment_info['f1_score'] = report['weighted avg']['f1-score']\n",
    "                experiment_info['precision'] = report['weighted avg']['precision']\n",
    "                experiment_info['recall'] = report['weighted avg']['recall']\n",
    "\n",
    "        metrics_data.append(experiment_info)\n",
    "\n",
    "    # Create dataframe for comparison\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Display comparison table\n",
    "    print(\"=== Experiment Comparison ===\")\n",
    "    print(df[['name', 'source', 'processing', 'model', 'accuracy', 'f1_score']])\n",
    "\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='name', y='accuracy', data=df)\n",
    "    plt.title('Test Accuracy Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='name', y='f1_score', data=df)\n",
    "    plt.title('F1 Score Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='name', y='precision', data=df)\n",
    "    plt.title('Precision Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x='name', y='recall', data=df)\n",
    "    plt.title('Recall Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save comparison plot\n",
    "    comparison_path = os.path.join(RESULTS_PATH, 'plots', f\"experiment_comparison.png\")\n",
    "    plt.savefig(comparison_path)\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "def display_comparison_controls():\n",
    "    \"\"\"Display controls for experiment comparison\"\"\"\n",
    "    experiments = list_experiments()\n",
    "\n",
    "    if not experiments:\n",
    "        print(\"No experiments found to compare.\")\n",
    "        return\n",
    "\n",
    "    # Create multi-select widget\n",
    "    experiment_selector = widgets.SelectMultiple(\n",
    "        options=experiments,\n",
    "        description='Experiments:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Compare button\n",
    "    compare_button = widgets.Button(\n",
    "        description='Compare Selected',\n",
    "        button_style='info',\n",
    "        icon='chart-bar'\n",
    "    )\n",
    "\n",
    "    def on_compare_click(b):\n",
    "        selected = experiment_selector.value\n",
    "        if not selected or len(selected) < 2:\n",
    "            print(\"Please select at least two experiments to compare.\")\n",
    "            return\n",
    "\n",
    "        compare_experiments(selected)\n",
    "\n",
    "    compare_button.on_click(on_compare_click)\n",
    "\n",
    "    # Display widgets\n",
    "    display(experiment_selector, compare_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i8PuZvkYaUK"
   },
   "source": [
    "## 12. Experiment Comparison Interface\n",
    "\n",
    "After you've run multiple experiments, use this section to compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "709f7862bb4d4823802d87fea9dac072",
      "97eb8b9caca24e57ace56adf12d8a19d",
      "e0b8aa99c72e4ee48e18fabc44acd6cc"
     ]
    },
    "id": "wwbs5CSKYbaB",
    "outputId": "16cad2df-c7c7-4538-a0b0-aa62689e859f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709f7862bb4d4823802d87fea9dac072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Show Experiment Comparison', icon='chart-bar', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No experiments found to compare.\n"
     ]
    }
   ],
   "source": [
    "# This button reveals the experiment comparison interface\n",
    "compare_experiments_button = widgets.Button(\n",
    "    description='Show Experiment Comparison',\n",
    "    button_style='info',\n",
    "    icon='chart-bar'\n",
    ")\n",
    "\n",
    "def on_show_comparison(b):\n",
    "    display_comparison_controls()\n",
    "\n",
    "compare_experiments_button.on_click(on_show_comparison)\n",
    "display(compare_experiments_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcHBtuppYdBZ"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "The notebook is now ready! To begin:\n",
    "\n",
    "1. First, select a data source from the dropdown above\n",
    "2. Then, choose a processing method\n",
    "3. Next, select a model architecture\n",
    "4. Configure hyperparameters\n",
    "5. Click \"Start Training\" to begin training\n",
    "6. After running multiple experiments, use the comparison tool to analyze results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0605ed1c631d46a2846a59fe990fece2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11946e19fd8c4612a67f1185dcb1210a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "original",
       "denoised",
       "enhanced",
       "sharpened"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Processing:",
      "description_tooltip": null,
      "disabled": false,
      "index": 0,
      "layout": "IPY_MODEL_9a4f3c8e22824b36b4b5ae995ac1d9d5",
      "style": "IPY_MODEL_aa227c2480c144d8b4a450eb13b0803e"
     }
    },
    "17727b275068471db85804b7100c022c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46df622d695c46c2b0d2d1d50f8bf960": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "575d210aab7242c78025d6d3facd2f19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "709f7862bb4d4823802d87fea9dac072": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "info",
      "description": "Show Experiment Comparison",
      "disabled": false,
      "icon": "chart-bar",
      "layout": "IPY_MODEL_97eb8b9caca24e57ace56adf12d8a19d",
      "style": "IPY_MODEL_e0b8aa99c72e4ee48e18fabc44acd6cc",
      "tooltip": ""
     }
    },
    "7cf69310f4bb46399e7f9ee06220096a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "success",
      "description": "Continue to Model Selection",
      "disabled": false,
      "icon": "arrow-right",
      "layout": "IPY_MODEL_0605ed1c631d46a2846a59fe990fece2",
      "style": "IPY_MODEL_7d760f0ce6504190a8f2d1d41bb773db",
      "tooltip": ""
     }
    },
    "7d760f0ce6504190a8f2d1d41bb773db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "8c819e3ded474b2097641ffed2034177": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96d4c0bc4de14d0bb5de873e82fc71e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "Implant Doctor.v1i.yolov5pytorch",
       "Dental Implants 2.0.v3i.yolov5pytorch",
       "implants.v2i.yolov5pytorch",
       "dental.v5i.yolov5pytorch",
       "IMPLANT SYSTEM DETECTION.v7i.yolov5pytorch"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Data Source:",
      "description_tooltip": null,
      "disabled": false,
      "index": 4,
      "layout": "IPY_MODEL_575d210aab7242c78025d6d3facd2f19",
      "style": "IPY_MODEL_a7a8ab20b9db4170bf5b6df48a7df7ae"
     }
    },
    "9753985ae17e4de1a86238b565cff1aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97eb8b9caca24e57ace56adf12d8a19d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a4f3c8e22824b36b4b5ae995ac1d9d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d906ccb068d420c87fb30e776bd3cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatLogSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatLogSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatLogSliderView",
      "base": 10,
      "continuous_update": false,
      "description": "Learning Rate:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_9753985ae17e4de1a86238b565cff1aa",
      "max": -1,
      "min": -5,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".3g",
      "step": 0.2,
      "style": "IPY_MODEL_fff2bc3bf80e4303b24324477233ccf1",
      "value": 0.01
     }
    },
    "9e7ab42970e64b0bbba98f5b73ef559d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7a8ab20b9db4170bf5b6df48a7df7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa227c2480c144d8b4a450eb13b0803e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be9bdccd653d40e18a91ae7c1eb4fa8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "efficientnetb3",
       "custom_cnn",
       "resnet50",
       "densenet121"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Model:",
      "description_tooltip": null,
      "disabled": false,
      "index": 1,
      "layout": "IPY_MODEL_9e7ab42970e64b0bbba98f5b73ef559d",
      "style": "IPY_MODEL_8c819e3ded474b2097641ffed2034177"
     }
    },
    "cf27a56e397f4a8e99443242f3dc95e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "primary",
      "description": "Select Source",
      "disabled": false,
      "icon": "check",
      "layout": "IPY_MODEL_17727b275068471db85804b7100c022c",
      "style": "IPY_MODEL_46df622d695c46c2b0d2d1d50f8bf960",
      "tooltip": ""
     }
    },
    "e0b8aa99c72e4ee48e18fabc44acd6cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "fff2bc3bf80e4303b24324477233ccf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
