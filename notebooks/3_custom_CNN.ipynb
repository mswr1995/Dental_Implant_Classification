{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNIJWZjoHCJWkfWvkOIVRCm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Custom CNN for Dental Implant Classification\n","\n","This notebook implements a custom CNN architecture for dental implant classification from radiographic images. The model is designed to identify different implant types based on visual features.\n","\n","## Key Features\n","- Custom CNN architecture designed for radiographic image analysis\n","- Data augmentation to improve model generalization\n","- Training with learning rate scheduling and early stopping\n","- Comprehensive evaluation metrics and visualizations"],"metadata":{"id":"PbqUDYlRgK8e"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNmBAuo0fBUH"},"outputs":[],"source":["# Install any additional packages (most are pre-installed in Colab)\n","!pip install tqdm\n","\n","# Check GPU availability\n","import tensorflow as tf\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"GPU Available:\", len(tf.config.list_physical_devices('GPU')) > 0)\n","!nvidia-smi  # Check GPU details"]},{"cell_type":"markdown","source":["## Data Preparation and Storage Setup\n","\n","This section mounts Google Drive for persistent storage and sets up the project directory structure."],"metadata":{"id":"6UT020Ougs2q"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Create project directories\n","import os\n","\n","# Project directories\n","ROOT_DIR = '/content/drive/MyDrive/dental_implant_project'\n","DATA_DIR = os.path.join(ROOT_DIR, 'data')\n","PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'data_processed')\n","TRAIN_DIR = os.path.join(PROCESSED_DATA_DIR, 'train')\n","VAL_DIR = os.path.join(PROCESSED_DATA_DIR, 'val')\n","TEST_DIR = os.path.join(PROCESSED_DATA_DIR, 'test')\n","RESULTS_DIR = os.path.join(ROOT_DIR, 'results')\n","MODEL_SAVE_DIR = os.path.join(RESULTS_DIR, 'models')\n","PLOTS_DIR = os.path.join(RESULTS_DIR, 'plots')\n","METRICS_DIR = os.path.join(RESULTS_DIR, 'metrics')\n","\n","# Create directories if they don't exist\n","os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n","os.makedirs(PLOTS_DIR, exist_ok=True)\n","os.makedirs(METRICS_DIR, exist_ok=True)\n","\n","print(\"Project directories set up successfully.\")"],"metadata":{"id":"f2kRPockfZXg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Verification and Import\n","\n","If your data is already in Google Drive, this will verify its structure.\n","If not, you can upload it directly to Colab using the uploader that will appear."],"metadata":{"id":"8iSlVJF0hBMN"}},{"cell_type":"code","source":["from google.colab import files\n","import zipfile\n","\n","# Upload dataset zip file\n","print(\"Please upload your dataset zip file...\")\n","uploaded = files.upload()\n","\n","# Extract the uploaded zip file\n","for filename in uploaded.keys():\n","    if filename.endswith('.zip'):\n","        with zipfile.ZipFile(filename, 'r') as zip_ref:\n","            zip_ref.extractall(PROCESSED_DATA_DIR)\n","        print(f\"Extracted {filename} to {PROCESSED_DATA_DIR}\")\n","\n","# Verify the dataset structure\n","!ls -la {TRAIN_DIR}\n","print(f\"Number of training classes: {len(os.listdir(TRAIN_DIR))}\")"],"metadata":{"id":"bKUHPn_gfbTs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Library Imports and Configuration\n","\n","Import all required libraries and set up configurations for training."],"metadata":{"id":"CEWQDem5hD7k"}},{"cell_type":"code","source":["# Import required libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import classification_report, confusion_matrix\n","import datetime\n","import random\n","import os\n","from IPython.display import display, Javascript\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","random.seed(42)\n","\n","# Enable mixed precision for faster training and lower memory usage\n","try:\n","    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","    tf.keras.mixed_precision.set_global_policy(policy)\n","    print(\"Mixed precision enabled\")\n","except:\n","    print(\"Mixed precision not available\")\n","\n","# Set image parameters\n","IMG_HEIGHT = 128  # Smaller size for custom CNN\n","IMG_WIDTH = 128\n","BATCH_SIZE = 32\n","\n","def keep_alive():\n","    display(Javascript('''\n","        function ClickConnect(){\n","            console.log(\"Keeping Colab connected\");\n","            document.querySelector(\"colab-toolbar-button#connect\").click()\n","        }\n","        setInterval(ClickConnect, 60000)\n","    '''))\n","\n","keep_alive()\n","print(\"Anti-disconnection measures enabled\")"],"metadata":{"id":"Dq3qRmCNfc8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Augmentation and Generators\n","\n","Create data generators with augmentation to improve model generalization."],"metadata":{"id":"wyvQC7EzhGOw"}},{"cell_type":"code","source":["# Data augmentation for training\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.1,\n","    zoom_range=0.1,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","# Minimal augmentation for validation and test data\n","valid_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Create generators\n","train_generator = train_datagen.flow_from_directory(\n","    TRAIN_DIR,\n","    target_size=(IMG_HEIGHT, IMG_WIDTH),\n","    batch_size=BATCH_SIZE,\n","    class_mode='categorical',\n","    shuffle=True\n",")\n","\n","valid_generator = valid_datagen.flow_from_directory(\n","    VAL_DIR,\n","    target_size=(IMG_HEIGHT, IMG_WIDTH),\n","    batch_size=BATCH_SIZE,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","test_generator = test_datagen.flow_from_directory(\n","    TEST_DIR,\n","    target_size=(IMG_HEIGHT, IMG_WIDTH),\n","    batch_size=BATCH_SIZE,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","# Get class information\n","classes = sorted(os.listdir(TRAIN_DIR))\n","num_classes = len(classes)\n","print(f\"Number of classes: {num_classes}\")"],"metadata":{"id":"Lfuua15offDC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Custom CNN Architecture Definition\n","\n","Define a custom CNN architecture specifically designed for dental implant classification."],"metadata":{"id":"7T-DTLEchKg3"}},{"cell_type":"code","source":["# Define the custom CNN model\n","def build_custom_cnn(input_shape=(512, 512, 3), num_classes=num_classes):\n","    model = Sequential([\n","        # First convolution block\n","        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n","        BatchNormalization(),\n","        Conv2D(32, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","\n","        # Second convolution block\n","        Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        Conv2D(64, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","\n","        # Third convolution block\n","        Conv2D(128, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        Conv2D(128, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","\n","        # Fourth convolution block\n","        Conv2D(256, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        Conv2D(256, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","\n","        # Fifth convolution block\n","        Conv2D(512, (3, 3), activation='relu', padding='same'),\n","        BatchNormalization(),\n","        GlobalAveragePooling2D(),\n","\n","        # Classification head\n","        Dropout(0.5),\n","        Dense(512, activation='relu'),\n","        BatchNormalization(),\n","        Dropout(0.3),\n","        Dense(num_classes, activation='softmax')\n","    ])\n","\n","    return model\n","\n","# Build the model\n","model = build_custom_cnn()\n","model.compile(\n","    optimizer=Adam(learning_rate=0.001),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Display model summary\n","model.summary()"],"metadata":{"id":"gk-KVNuifguZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training\n","\n","Train the custom CNN with callbacks for checkpointing, early stopping, and learning rate reduction."],"metadata":{"id":"dPH8JOI4hMFw"}},{"cell_type":"code","source":["# Set up callbacks\n","checkpoint_path = os.path.join(MODEL_SAVE_DIR, 'custom_cnn_best_model.h5')\n","callbacks = [\n","    ModelCheckpoint(\n","        checkpoint_path,\n","        monitor='val_accuracy',\n","        verbose=1,\n","        save_best_only=True,\n","        mode='max'\n","    ),\n","    EarlyStopping(\n","        monitor='val_accuracy',\n","        patience=10,\n","        restore_best_weights=True\n","    ),\n","    ReduceLROnPlateau(\n","        monitor='val_loss',\n","        factor=0.2,\n","        patience=5,\n","        min_lr=1e-6\n","    )\n","]\n","\n","# Train the model\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n","    epochs=50,\n","    validation_data=valid_generator,\n","    validation_steps=valid_generator.samples // BATCH_SIZE,\n","    callbacks=callbacks\n",")\n","\n","# Save training history\n","history_df = pd.DataFrame(history.history)\n","history_df.to_csv(os.path.join(METRICS_DIR, 'custom_cnn_training_history.csv'), index=False)"],"metadata":{"id":"gkfUPlWTfiSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Evaluation\n","\n","Evaluate the trained model using accuracy, confusion matrix, and classification report."],"metadata":{"id":"AdxvvMObhTjd"}},{"cell_type":"code","source":["# Evaluate the model on the test set\n","test_loss, test_accuracy = model.evaluate(test_generator)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Loss: {test_loss:.4f}\")\n","\n","# Generate predictions\n","test_generator.reset()\n","y_pred_probs = model.predict(test_generator)\n","y_pred = np.argmax(y_pred_probs, axis=1)\n","y_true = test_generator.classes\n","\n","# Classification report\n","class_names = list(test_generator.class_indices.keys())\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_true, y_pred, target_names=class_names))\n","\n","# Confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()"],"metadata":{"id":"vplTK491fkAc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save the model\n","\n","Save the final model\n"],"metadata":{"id":"6NpTsdbalw_6"}},{"cell_type":"code","source":["# Save the final model\n","final_model_path = os.path.join(MODEL_SAVE_DIR, 'custom_cnn_final_model.h5')\n","model.save(final_model_path)\n","print(f\"Model saved to {final_model_path}\")\n","\n","# Zip results for download\n","!zip -r /content/custom_cnn_results.zip {RESULTS_DIR}\n","from google.colab import files\n","files.download('/content/custom_cnn_results.zip')"],"metadata":{"id":"viaOMouzflp8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Summary and Conclusions\n","\n","This notebook implemented a custom CNN architecture for dental implant classification. The model was designed with multiple convolutional blocks followed by batch normalization and dropout layers to prevent overfitting.\n","\n","### Key Findings:\n","- The custom CNN achieved [X]% accuracy on the test set\n","- Top-3 accuracy was [Y]%, indicating good performance even when the model's first prediction is wrong\n","- Common misclassifications occurred between visually similar implant types\n","\n","### Comparison with EfficientNetB3:\n","- The custom CNN is [lighter/heavier] than EfficientNetB3\n","- Training time was [shorter/longer]\n","- Performance was [better/worse] in terms of accuracy\n","\n","### Next Steps:\n","1. Ensemble both models for potentially better performance\n","2. Apply explainability techniques to understand decision-making\n","3. Test the model on external validation data\n","4. Deploy the model for clinical testing"],"metadata":{"id":"OAi3-cWbfm8l"}}]}